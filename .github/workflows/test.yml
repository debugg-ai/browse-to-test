name: Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests weekly on Sunday at 2 AM UTC
    - cron: '0 2 * * 0'

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.8", "3.9", "3.10", "3.11"]
        exclude:
          # Skip some combinations to reduce CI time
          - os: windows-latest
            python-version: "3.8"
          - os: macos-latest
            python-version: "3.8"

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -e .

    - name: Lint with flake8
      run: |
        # Stop the build if there are Python syntax errors or undefined names
        flake8 browse_to_test --count --select=E9,F63,F7,F82 --show-source --statistics
        # Exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        flake8 browse_to_test tests --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

    - name: Type checking with mypy
      run: |
        mypy browse_to_test --ignore-missing-imports
      continue-on-error: true  # Don't fail CI on type checking issues for now

    - name: Security check with bandit
      run: |
        bandit -r browse_to_test -f json -o bandit-report.json || true
        bandit -r browse_to_test
      continue-on-error: true  # Don't fail CI on security issues for now

    - name: Run tests with pytest
      env:
        # Use mock API keys for testing (AI providers will be mocked)
        OPENAI_API_KEY: sk-test-key-for-mocking
        ANTHROPIC_API_KEY: sk-ant-test-key-for-mocking
      run: |
        pytest \
          --cov=browse_to_test \
          --cov-report=xml \
          --cov-report=term-missing \
          --cov-fail-under=80 \
          --junit-xml=pytest-report.xml \
          -v

    - name: Upload coverage reports to Codecov
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: true

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-results-${{ matrix.os }}-${{ matrix.python-version }}
        path: |
          pytest-report.xml
          coverage.xml
          bandit-report.json

  test-with-ai:
    # Run a subset of tests with real AI providers (if API keys are available)
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || contains(github.event.head_commit.message, '[test-ai]')
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -e .

    - name: Run AI provider tests with real APIs
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      run: |
        # Only run AI tests if API keys are available
        if [ -n "$OPENAI_API_KEY" ] || [ -n "$ANTHROPIC_API_KEY" ]; then
          pytest tests/test_ai_providers.py -m "network" -v --tb=short
        else
          echo "Skipping AI provider tests - no API keys available"
        fi
      continue-on-error: true  # Don't fail CI if AI tests fail

  integration-test:
    # Run integration tests on a single environment
    runs-on: ubuntu-latest
    needs: test
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -e .

    - name: Run integration tests
      run: |
        pytest -m "integration" -v --tb=short
      env:
        OPENAI_API_KEY: sk-test-key-for-mocking
        ANTHROPIC_API_KEY: sk-ant-test-key-for-mocking

    - name: Run end-to-end tests
      run: |
        # Test the command-line interface and examples
        python examples/basic_usage.py
        python examples/context_aware_example.py
      continue-on-error: true  # Don't fail if examples have issues

  performance-test:
    # Run performance benchmarks
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || contains(github.event.head_commit.message, '[test-perf]')
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install pytest-benchmark
        pip install -e .

    - name: Run performance tests
      run: |
        pytest tests/ -m "not slow" --benchmark-only --benchmark-json=benchmark.json
      continue-on-error: true

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: benchmark.json

  docs-test:
    # Test documentation and examples
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -e .

    - name: Check code formatting with black
      run: |
        black --check browse_to_test tests
      continue-on-error: true  # Don't fail CI on formatting for now

    - name: Check import sorting with isort
      run: |
        isort --check-only browse_to_test tests
      continue-on-error: true  # Don't fail CI on import sorting for now

    - name: Validate documentation examples
      run: |
        # Check that README examples are valid Python
        python -m py_compile examples/*.py
        
        # Test doctests if any exist
        python -m doctest browse_to_test/*.py || true

    - name: Test package installation
      run: |
        # Test that package can be installed and imported
        pip uninstall -y browse-to-test
        pip install .
        python -c "import browse_to_test; print('Package imports successfully')"
        python -c "from browse_to_test import TestScriptOrchestrator; print('Main classes import successfully')"

  notify:
    # Send notifications about test results
    runs-on: ubuntu-latest
    needs: [test, integration-test]
    if: always() && (github.event_name == 'schedule' || github.ref == 'refs/heads/main')
    steps:
    - name: Notify on failure
      if: needs.test.result == 'failure' || needs.integration-test.result == 'failure'
      run: |
        echo "Tests failed! Check the workflow run for details."
        # Here you could add Slack, Discord, or email notifications
        
    - name: Notify on success
      if: needs.test.result == 'success' && needs.integration-test.result == 'success'
      run: |
        echo "All tests passed successfully!" 